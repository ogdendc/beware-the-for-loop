# beware-the-for-loop
a comparison of sequential vs parallel group-by data processing

For anyone with a coding background, writing “loops” is a common practice.  A loop is a process for iterating and repeating a block of code.  Such a programming tool has many applications.  When I was in college and using Fortran (yes, I’m not a youngling) I was using do-loops.  During my career, I’ve learned new programming and scripting languages; and now find myself frequently using for-loops.  In my job, I get to interact with many customers, and sometimes that leads to me seeing their code.  What I see is a lot of for-loops out there in the world. 

But here’s the problem:  for-loops are a serial (not parallel) process.  Why does that matter?  In our brave new world of Big Data, I think it’s safe to say that parallel processing is paramount.  If you’re working in the world of small data, then serial processing probably works fine.  Or does it?  Some of the customers I work with are writing code to process what could be considered small data, and yet are often using tools built specifically to handle Big Data; and they are perplexed as to why their for-loop is running for hours or even days.  I’ve worked with several customers who, when faced with this scenario, try to solve it by simply throwing more powerful compute (clusters of machines with CPUs and memory) at their long-running for-loop, and are perplexed because more parallel computing power doesn’t actually solve their problem.

What is the “insidious” type of for-loop?  One that iterates through subsets of rows in a dataframe, and independently processes each subset.  For example, suppose one column in a dataframe is “geography” which indicates various locations for a retail company.  A common use of a for-loop would be to iterate through each geography, and process the data for each geography separately.  There are many applications for such an approach.  For example, we may want to train machine learning models that are specific to each geography.

There is a common objection that I’ve heard, to the idea of converting existing non-parallelized processing into something that is more “sparkified”:  when the customer or colleague is using Pandas, and knowing that Pandas is not a distributed-computing package, the objection is a lack of appetite for rewriting their existing code from Python + Pandas into Python + Spark (sans Pandas).  Rest assured, using Pandas does not stand in your way of parallelizing your process.  This is demonstrated in the accompanying notebook/code.

The accompanying notebook/code demonstrates the insidious type of for-loop (one that iterates through subsets of the data and independently processes each subset), while also demonstrating much faster alternative approaches, thanks to the parallel processing power of Spark.

